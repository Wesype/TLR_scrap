#!/usr/bin/env python3
"""
Script principal - Extraction automatique des notifications T√©l√©recours

Usage:
    python main.py                    # Mode interactif
    python main.py --auto             # Mode automatique (toutes les juridictions avec notifs)
    python main.py --juridiction TA78 # Une juridiction sp√©cifique
"""

import asyncio
import argparse
import getpass
import time
import os
import json
from pathlib import Path
from crawl4ai import AsyncWebCrawler, BrowserConfig

from config import TelecoursConfig
from auth import TelecoursAuth
from notifs import NotificationDetector
from scraper_messages import MessageScraper
from utils import print_header, print_summary, compte_pdfs_dossier, send_webhook


async def envoyer_resultats_webhook(config: TelecoursConfig, juridictions: list):
    """Envoie les r√©sultats de toutes les juridictions vers le webhook"""
    
    print("\nüì§ Envoi des r√©sultats vers le webhook...")
    
    tous_les_messages = []
    
    for juridiction in juridictions:
        json_file = config.get_juridiction_dir(juridiction.code) / f"messages_{juridiction.code}.json"
        
        if json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                messages = json.load(f)
                
                # Ajouter le code juridiction √† chaque message
                for msg in messages:
                    msg['code_juridiction'] = juridiction.code
                    msg['nom_juridiction'] = juridiction.nom
                
                tous_les_messages.extend(messages)
    
    if tous_les_messages:
        payload = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'nb_juridictions': len(juridictions),
            'nb_messages_total': len(tous_les_messages),
            'messages': tous_les_messages
        }
        
        send_webhook(config.webhook_url, payload)
    else:
        print("‚ö†Ô∏è  Aucun message √† envoyer")


async def main_auto(config: TelecoursConfig):
    """
    Mode automatique : extrait tous les messages de toutes les juridictions avec notifs
    """
    
    print_header("ü§ñ MODE AUTOMATIQUE - EXTRACTION COMPL√àTE")
    
    # Configuration du navigateur
    browser_config = BrowserConfig(
        headless=config.headless,
        verbose=False,
        viewport_width=1920,
        viewport_height=1080,
        accept_downloads=True,
        downloads_path=str(config.pdfs_dir.absolute())
    )
    
    start_time = time.time()
    total_messages = 0
    total_pdfs = 0
    juridictions_traitees = 0
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        # Authentification
        auth = TelecoursAuth(config)
        await auth.setup_cookie_hook(crawler)
        
        if not await auth.login(crawler):
            return
        
        # D√©tection des notifications
        detector = NotificationDetector(config)
        juridictions = await detector.get_juridictions_avec_notifs(crawler)
        
        if not juridictions:
            print("\nüì≠ Aucune notification trouv√©e")
            return
        
        await detector.afficher_juridictions_avec_notifs(juridictions)
        
        # Demander confirmation
        reponse = input(f"\n‚ùì Extraire les messages de ces {len(juridictions)} juridictions ? (o/N) : ")
        if reponse.lower() != 'o':
            print("‚ùå Extraction annul√©e")
            return
        
        # Traiter chaque juridiction
        scraper = MessageScraper(config, auth.cookies)
        
        for i, juridiction in enumerate(juridictions, 1):
            print(f"\n{'='*70}")
            print(f"üìç Juridiction {i}/{len(juridictions)}: {juridiction.code} ({juridiction.nom})")
            print(f"   {juridiction.nb_notifs} message(s) non lu(s)")
            print(f"{'='*70}")
            
            # S√©lectionner la juridiction
            if not await detector.selectionner_juridiction(crawler, juridiction):
                print(f"‚ö†Ô∏è  Impossible de s√©lectionner {juridiction.code}, on passe √† la suivante")
                continue
            
            # Scraper les messages NON LUS
            messages = await scraper.scraper_tous_messages(
                crawler=crawler,
                code_juridiction=juridiction.code,
                messages_non_lus_seulement=not config.scraper_messages_lus,
                max_messages=config.max_messages_par_juridiction
            )
            
            if messages:
                total_messages += len(messages)
                juridictions_traitees += 1
                
                # Compter les PDFs
                pdfs_dir = config.get_pdfs_dir(juridiction.code)
                nb_pdfs = compte_pdfs_dossier(pdfs_dir)
                total_pdfs += nb_pdfs
                
                print(f"\n   ‚úÖ {len(messages)} message(s) extrait(s)")
                print(f"   üì• {nb_pdfs} PDF(s) t√©l√©charg√©(s)")
        
        # R√©sum√© final
        duration = time.time() - start_time
        print_summary(juridictions_traitees, total_messages, total_pdfs, duration)
        
        # Les messages ont d√©j√† √©t√© envoy√©s individuellement au webhook pendant le scraping
        print("\n‚úÖ Tous les messages ont √©t√© envoy√©s au webhook individuellement")
        
        await crawler.crawler_strategy.kill_session(config.session_id)


async def main_juridiction(config: TelecoursConfig, code_juridiction: str):
    """
    Mode juridiction sp√©cifique
    """
    
    print_header(f"üìç EXTRACTION - {code_juridiction}")
    
    browser_config = BrowserConfig(
        headless=config.headless,
        verbose=False,
        viewport_width=1920,
        viewport_height=1080,
        accept_downloads=True,
        downloads_path=str(config.pdfs_dir.absolute())
    )
    
    start_time = time.time()
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        # Authentification
        auth = TelecoursAuth(config)
        await auth.setup_cookie_hook(crawler)
        
        if not await auth.login(crawler):
            return
        
        # D√©tection des notifications
        detector = NotificationDetector(config)
        juridictions = await detector.get_juridictions_avec_notifs(crawler)
        
        # Chercher la juridiction demand√©e
        juridiction_cible = next(
            (j for j in juridictions if j.code == code_juridiction),
            None
        )
        
        if not juridiction_cible:
            print(f"\n‚ö†Ô∏è  Juridiction {code_juridiction} non trouv√©e ou sans notification")
            return
        
        print(f"\nüìç {juridiction_cible.code} - {juridiction_cible.nom}")
        print(f"   {juridiction_cible.nb_notifs} message(s) non lu(s)")
        
        # S√©lectionner la juridiction
        if not await detector.selectionner_juridiction(crawler, juridiction_cible):
            return
        
        # Scraper les messages
        scraper = MessageScraper(config, auth.cookies)
        messages = await scraper.scraper_tous_messages(
            crawler=crawler,
            code_juridiction=code_juridiction,
            messages_non_lus_seulement=not config.scraper_messages_lus,
            max_messages=config.max_messages_par_juridiction
        )
        
        # R√©sum√©
        duration = time.time() - start_time
        nb_pdfs = compte_pdfs_dossier(config.get_pdfs_dir(code_juridiction))
        
        print_summary(1, len(messages) if messages else 0, nb_pdfs, duration)
        
        # Les messages ont d√©j√† √©t√© envoy√©s individuellement au webhook pendant le scraping
        if config.webhook_url and messages:
            print(f"\n‚úÖ {len(messages)} message(s) envoy√©(s) au webhook individuellement")
        
        await crawler.crawler_strategy.kill_session(config.session_id)


async def main_interactif(config: TelecoursConfig):
    """
    Mode interactif : affiche les juridictions et laisse l'utilisateur choisir
    """
    
    print_header("üí¨ MODE INTERACTIF")
    
    browser_config = BrowserConfig(
        headless=config.headless,
        verbose=False,
        viewport_width=1920,
        viewport_height=1080,
        accept_downloads=True,
        downloads_path=str(config.pdfs_dir.absolute())
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        # Authentification
        auth = TelecoursAuth(config)
        await auth.setup_cookie_hook(crawler)
        
        if not await auth.login(crawler):
            return
        
        # D√©tection des notifications
        detector = NotificationDetector(config)
        juridictions = await detector.get_juridictions_avec_notifs(crawler)
        
        if not juridictions:
            print("\nüì≠ Aucune notification trouv√©e")
            return
        
        await detector.afficher_juridictions_avec_notifs(juridictions)
        
        # Menu de choix
        print("Options:")
        print("  1. Extraire TOUTES les juridictions")
        print("  2. Choisir UNE juridiction sp√©cifique")
        print("  0. Quitter")
        
        choix = input("\nVotre choix : ").strip()
        
        if choix == "0":
            print("üëã Au revoir !")
            return
        
        elif choix == "1":
            # Extraire toutes
            scraper = MessageScraper(config, auth.cookies)
            start_time = time.time()
            total_messages = 0
            total_pdfs = 0
            
            for i, juridiction in enumerate(juridictions, 1):
                print(f"\n{'='*70}")
                print(f"üìç Juridiction {i}/{len(juridictions)}: {juridiction.code}")
                print(f"{'='*70}")
                
                if not await detector.selectionner_juridiction(crawler, juridiction):
                    continue
                
                messages = await scraper.scraper_tous_messages(
                    crawler=crawler,
                    code_juridiction=juridiction.code,
                    messages_non_lus_seulement=not config.scraper_messages_lus,
                    max_messages=config.max_messages_par_juridiction
                )
                
                if messages:
                    total_messages += len(messages)
                    nb_pdfs = compte_pdfs_dossier(config.get_pdfs_dir(juridiction.code))
                    total_pdfs += nb_pdfs
            
            duration = time.time() - start_time
            print_summary(len(juridictions), total_messages, total_pdfs, duration)
        
        elif choix == "2":
            # Choisir une juridiction
            print("\nCodes disponibles:")
            for j in juridictions:
                print(f"  - {j.code} ({j.nom})")
            
            code = input("\nCode juridiction : ").strip().upper()
            
            juridiction_cible = next((j for j in juridictions if j.code == code), None)
            
            if not juridiction_cible:
                print(f"‚ùå Juridiction {code} non trouv√©e")
                return
            
            if not await detector.selectionner_juridiction(crawler, juridiction_cible):
                return
            
            scraper = MessageScraper(config, auth.cookies)
            start_time = time.time()
            
            messages = await scraper.scraper_tous_messages(
                crawler=crawler,
                code_juridiction=code,
                messages_non_lus_seulement=not config.scraper_messages_lus,
                max_messages=config.max_messages_par_juridiction
            )
            
            duration = time.time() - start_time
            nb_pdfs = compte_pdfs_dossier(config.get_pdfs_dir(code))
            
            print_summary(1, len(messages) if messages else 0, nb_pdfs, duration)
            
            # Envoyer webhook si configur√©
            if config.webhook_url:
                await envoyer_resultats_webhook(config, [juridiction_cible])
        
        await crawler.crawler_strategy.kill_session(config.session_id)


def main():
    """Point d'entr√©e"""
    
    parser = argparse.ArgumentParser(
        description="Extracteur de messages T√©l√©recours avec d√©tection automatique des notifications"
    )
    parser.add_argument(
        '--auto',
        action='store_true',
        help="Mode automatique : extrait toutes les juridictions avec notifications"
    )
    parser.add_argument(
        '--juridiction',
        type=str,
        help="Code d'une juridiction sp√©cifique (ex: TA78)"
    )
    parser.add_argument(
        '--no-headless',
        action='store_true',
        help="D√©sactiver le mode headless (afficher le navigateur)"
    )
    parser.add_argument(
        '--messages-lus',
        action='store_true',
        help="Scraper les messages lus au lieu des non lus (pour tests)"
    )
    parser.add_argument(
        '--max-messages',
        type=int,
        default=None,
        help="Nombre maximum de messages par juridiction (sera demand√© si non sp√©cifi√©)"
    )
    parser.add_argument(
        '--username',
        type=str,
        help="Identifiant T√©l√©recours (sinon lu depuis TELERECOURS_USERNAME ou demand√©)"
    )
    parser.add_argument(
        '--password',
        type=str,
        help="Mot de passe T√©l√©recours (sinon lu depuis TELERECOURS_PASSWORD ou demand√©)"
    )
    parser.add_argument(
        '--webhook',
        type=str,
        help="URL du webhook pour envoyer les r√©sultats JSON"
    )
    
    args = parser.parse_args()
    
    # Configuration
    config = TelecoursConfig(
        headless=not args.no_headless,  # headless par d√©faut, sauf si --no-headless
        max_messages_par_juridiction=args.max_messages,
        scraper_messages_lus=args.messages_lus,
        webhook_url=args.webhook
    )
    
    # Demander les identifiants
    print("=" * 70)
    print("üîê IDENTIFIANTS T√âL√âRECOURS")
    print("=" * 70)
    
    # Priorit√© : argument CLI > variable d'environnement > input interactif
    config.username = args.username or os.environ.get('TELERECOURS_USERNAME')
    if not config.username:
        config.username = input("Identifiant: ").strip()
    else:
        source = "argument CLI" if args.username else "variable d'environnement"
        print(f"Identifiant: {config.username} (depuis {source})")
    
    if not config.username:
        print("‚ùå Identifiant requis")
        return
    
    config.password = args.password or os.environ.get('TELERECOURS_PASSWORD')
    if not config.password:
        config.password = getpass.getpass("Mot de passe: ").strip()
    else:
        source = "argument CLI" if args.password else "variable d'environnement"
        print(f"Mot de passe: *** (depuis {source})")
    
    if not config.password:
        print("‚ùå Mot de passe requis")
        return
    
    # Demander le nombre de messages si non sp√©cifi√©
    if args.max_messages is None:
        try:
            nb_messages = input("Nombre de messages √† scraper (d√©faut: 10) : ").strip()
            args.max_messages = int(nb_messages) if nb_messages else 10
        except ValueError:
            args.max_messages = 10
    
    config.max_messages_par_juridiction = args.max_messages
    
    if args.messages_lus:
        print(f"‚ö†Ô∏è  Mode TEST : scraping des messages LUS (pas de d√©sactivation des notifs)")
    
    # Lancer le mode appropri√©
    if args.auto:
        asyncio.run(main_auto(config))
    elif args.juridiction:
        asyncio.run(main_juridiction(config, args.juridiction.upper()))
    else:
        asyncio.run(main_interactif(config))


if __name__ == "__main__":
    main()