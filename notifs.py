"""
Module de d√©tection des notifications par juridiction
"""

import re
from typing import List, Dict
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

from config import TelecoursConfig


class JuridictionNotification:
    """Repr√©sente une juridiction avec ses notifications"""
    
    def __init__(self, code: str, nom: str, nb_notifs: int, event_target: str, event_argument: str):
        self.code = code
        self.nom = nom
        self.nb_notifs = nb_notifs
        self.event_target = event_target
        self.event_argument = event_argument
    
    def __repr__(self):
        return f"<Juridiction {self.code} ({self.nom}): {self.nb_notifs} notification(s)>"


class NotificationDetector:
    """D√©tecte les juridictions avec des notifications"""
    
    def __init__(self, config: TelecoursConfig):
        self.config = config
    
    async def get_juridictions_avec_notifs(
        self, 
        crawler: AsyncWebCrawler,
        html_selection: str = None
    ) -> List[JuridictionNotification]:
        """
        Extrait les juridictions avec notifications depuis la page de s√©lection
        
        Args:
            crawler: Instance du crawler
            html_selection: HTML de la page de s√©lection (optionnel)
        
        Returns:
            List[JuridictionNotification]: Liste des juridictions avec notifs
        """
        
        # Si pas d'HTML fourni, on r√©cup√®re la page
        if not html_selection:
            print("üîç R√©cup√©ration de la page de s√©lection des juridictions...")
            config = CrawlerRunConfig(
                session_id=self.config.session_id,
                page_timeout=self.config.page_timeout,
                cache_mode=0,
                verbose=False
            )
            
            result = await crawler.arun(
                url=self.config.selection_juridiction_url,
                config=config
            )
            
            if not result.success:
                print(f"‚ùå Erreur: {result.error_message}")
                return []
            
            html_selection = result.html
        
        # Parser le HTML
        soup = BeautifulSoup(html_selection, 'html.parser')
        
        # Trouver toutes les juridictions avec notifications
        # Structure : <li name="TA75"><a href="...">Paris<span class="page-choixJuridiction-mail"><span>2</span></span></a></li>
        juridictions_avec_notifs = []
        
        # Chercher tous les <li> avec attribut name
        for li in soup.find_all('li', attrs={'name': True}):
            code_juridiction = li.get('name')
            
            # Trouver le lien
            link = li.find('a')
            if not link:
                continue
            
            # Extraire le nom de la juridiction (texte avant le span)
            nom_juridiction = link.get_text(strip=True)
            
            # Chercher le span avec les notifications
            span_notif = link.find('span', class_='page-choixJuridiction-mail')
            
            if span_notif:
                # R√©cup√©rer le nombre de notifications
                span_nombre = span_notif.find('span')
                nb_notifs = int(span_nombre.get_text(strip=True)) if span_nombre else 0
                
                # Le nom est avant le span de notification
                nom_parts = []
                for content in link.contents:
                    if isinstance(content, str):
                        nom_parts.append(content.strip())
                    elif content.name == 'span':
                        break
                nom_juridiction = ''.join(nom_parts).strip()
            else:
                # Pas de notification
                nb_notifs = 0
            
            # Extraire les param√®tres du PostBack
            href = link.get('href', '')
            match = re.search(r"__doPostBack\('([^']+)','([^']+)'\)", href)
            
            if match:
                event_target = match.group(1)
                event_argument = match.group(2)
                
                # Cr√©er l'objet juridiction
                juridiction = JuridictionNotification(
                    code=code_juridiction,
                    nom=nom_juridiction,
                    nb_notifs=nb_notifs,
                    event_target=event_target,
                    event_argument=event_argument
                )
                
                # Ajouter uniquement si notifications > 0
                if nb_notifs > 0:
                    juridictions_avec_notifs.append(juridiction)
        
        return juridictions_avec_notifs
    
    async def afficher_juridictions_avec_notifs(
        self, 
        juridictions: List[JuridictionNotification]
    ):
        """Affiche un r√©sum√© des juridictions avec notifications"""
        
        if not juridictions:
            print("\nüì≠ Aucune notification trouv√©e")
            return
        
        print(f"\nüì¨ {len(juridictions)} juridiction(s) avec notifications :")
        print("=" * 70)
        
        total_notifs = sum(j.nb_notifs for j in juridictions)
        
        for juridiction in sorted(juridictions, key=lambda x: x.nb_notifs, reverse=True):
            print(f"   üìç {juridiction.code} - {juridiction.nom:<30} : {juridiction.nb_notifs:>3} message(s)")
        
        print("=" * 70)
        print(f"   TOTAL: {total_notifs} message(s) non lu(s)")
        print()
    
    async def selectionner_juridiction(
        self,
        crawler: AsyncWebCrawler,
        juridiction: JuridictionNotification
    ) -> bool:
        """
        S√©lectionne une juridiction
        
        Args:
            crawler: Instance du crawler
            juridiction: Juridiction √† s√©lectionner
        
        Returns:
            bool: True si s√©lection r√©ussie
        """
        
        if self.config.verbose:
            print(f"üìã S√©lection de {juridiction.code} ({juridiction.nom})...")
        
        js_juridiction = f"__doPostBack('{juridiction.event_target}', '{juridiction.event_argument}');"
        
        config_juridiction = CrawlerRunConfig(
            session_id=self.config.session_id,
            js_code=js_juridiction,
            js_only=True,
            wait_for="css:body",
            page_timeout=self.config.page_timeout,
            cache_mode=0,
            verbose=False
        )
        
        result = await crawler.arun(
            url=self.config.selection_juridiction_url,
            config=config_juridiction
        )
        
        if not result.success:
            print(f"‚ùå Erreur s√©lection {juridiction.code}: {result.error_message}")
            return False
        
        if self.config.verbose:
            print(f"   ‚úÖ {juridiction.code} s√©lectionn√©")
        
        return True